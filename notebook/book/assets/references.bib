@article{kleykoSurveyHyperdimensionalComputing2021,
  title = {A {{Survey}} on {{Hyperdimensional Computing}} Aka {{Vector Symbolic Architectures}}, {{Part I}}: {{Models}} and {{Data Transformations}}},
  shorttitle = {A {{Survey}} on {{Hyperdimensional Computing}} Aka {{Vector Symbolic Architectures}}, {{Part I}}},
  author = {Kleyko, Denis and Rachkovskij, Dmitri A. and Osipov, Evgeny and Rahimi, Abbas},
  year = {2021},
  month = nov,
  journal = {arXiv:2111.06077 [cs]},
  eprint = {2111.06077},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {This two-part comprehensive survey is devoted to a computing framework most commonly known under the names Hyperdimensional Computing and Vector Symbolic Architectures (HDC/VSA). Both names refer to a family of computational models that use high-dimensional distributed representations and rely on the algebraic properties of their key operations to incorporate the advantages of structured symbolic representations and vector distributed representations. Notable models in the HDC/VSA family are Tensor Product Representations, Holographic Reduced Representations, Multiply-Add-Permute, Binary Spatter Codes, and Sparse Binary Distributed Representations but there are other models too. HDC/VSA is a highly interdisciplinary area with connections to computer science, electrical engineering, artificial intelligence, mathematics, and cognitive science. This fact makes it challenging to create a thorough overview of the area. However, due to a surge of new researchers joining the area in recent years, the necessity for a comprehensive survey of the area has become extremely important. Therefore, amongst other aspects of the area, this Part I surveys important aspects such as: known computational models of HDC/VSA and transformations of various input data types to high-dimensional distributed representations. Part II of this survey is devoted to applications, cognitive computing and architectures, as well as directions for future work. The survey is written to be useful for both newcomers and practitioners.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/ross/Zotero/storage/BDJXNWIR/Kleyko et al. - 2021 - A Survey on Hyperdimensional Computing aka Vector .pdf;/home/ross/Zotero/storage/IS5X4IXI/2111.html}
}





@phdthesis{plateDistributedRepresentationsNested1994,
  type = {Doctor of Philosophy},
  title = {Distributed {{Representations}} and {{Nested Compositional Structure}}},
  author = {Plate, Tony A.},
  year = {1994},
  abstract = {Distributed representations are attractive for a number of reasons. They offer the pos- sibility of representing concepts in a continuous space, they degrade gracefully with noise, and they can be processed in a parallel network of simple processing elements. However, the problem of representing nested structure in distributed representations has been for some time aprominent concern of both proponents and critics of connectionism [Fodor and Pylyshyn 1988; Smolensky 1990; Hinton 1990]. The lack of connectionist representations for complex structure has held back progress in tackling higher-level cognitive tasks such as language understanding and reasoning. In this thesis I review connectionist representations and propose a method for the distributed representation of nested structure, which I call ``Holographic Reduced Rep- resentations'' (HRRs). HRRs provide an implementation of Hinton's [1990] ``reduced descriptions''. HRRs use circular convolution to associate atomic items, which are repre- sented by vectors. Arbitrary variable bindings, short sequences of various lengths, and predicates can be represented in a fixed-width vector. These representations are items in their own right, and can be used in constructing compositional structures. The noisy re- constructions extracted from convolution memories can be cleaned up by using a separate associative memory that has good reconstructive properties. Circular convolution, which is the basic associative operator for HRRs, can be built into a recurrent neural network. The network can store and produce sequences. I show that neural network learning techniques can be used with circular convolution in order to learn representations for items and sequences. Oneof the attractions of connectionist representations of compositional structures is the possibility of computing without decomposing structures. I show that it is possible to use dot-product comparisons ofHRRsfor nested structures to estimate the analogical similarity of the structures. This demonstrates how the surface form of connectionist representations can reflect underlying structural similarity and alignment.},
  school = {University of Toronto},
  keywords = {cogsci},
  file = {/home/ross/Zotero/storage/33UV74N5/Plate - 1994 - Distributed Representations and Nested Compositional Structure.pdf}
}



@inproceedings{noestPhasorNeuralNetworks1987,
  title = {Phasor Neural Networks},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Neural Information Processing Systems}}, {{Natural}} and {{Synthetic}},},
  author = {Noest, J.},
  year = {1987},
  month = dec,
  pages = {584--591},
  abstract = {A novel network type is introduced which uses unit-length 2-vectors for local variables. As an example of its applications, associative memory nets are defined and their performance analyzed. Real systems corresponding to such 'phasor' models can be e.g. (neuro)biological networks of limit-cycle oscillators or optical resonators that have a hologram in their feedback path.},
  langid = {english},
  file = {/home/ross/Zotero/storage/SAE2YDNU/Noest - 1987 - Phasor neural networks.pdf}
}





@article{fradyRobustComputationRhythmic2019,
  title = {Robust Computation with Rhythmic Spike Patterns},
  author = {Frady, E. Paxon and Sommer, Friedrich T.},
  year = {2019},
  month = sep,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {116},
  number = {36},
  pages = {18050--18059},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1902653116},
  abstract = {Information coding by precise timing of spikes can be faster and more energy efficient than traditional rate coding. However , spike-timing codes are often brittle, which has limited their use in theoretical neuroscience and computing applications. Here, we propose a type of attractor neural network in complex state space and show how it can be leveraged to construct spiking neural networks with robust computational properties through a phase-to-timing mapping. Building on Hebbian neural associa-tive memories, like Hopfield networks, we first propose threshold phasor associative memory (TPAM) networks. Complex phasor patterns whose components can assume continuous-valued phase angles and binary magnitudes can be stored and retrieved as stable fixed points in the network dynamics. TPAM achieves high memory capacity when storing sparse phasor patterns, and we derive the energy function that governs its fixed-point attrac-tor dynamics. Second, we construct 2 spiking neural networks to approximate the complex algebraic computations in TPAM, a reductionist model with resonate-and-fire neurons and a biologically plausible network of integrate-and-fire neurons with synaptic delays and recurrently connected inhibitory interneurons. The fixed points of TPAM correspond to stable periodic states of precisely timed spiking activity that are robust to perturbation. The link established between rhythmic firing patterns and complex attractor dynamics has implications for the interpretation of spike patterns seen in neuroscience and can serve as a framework for computation in emerging neuromorphic devices.},
  langid = {english},
  file = {/home/ross/Zotero/storage/D8UGPHA9/Frady and Sommer - 2019 - Robust computation with rhythmic spike patterns.pdf}
}





@article{schlegelComparisonVectorSymbolic2021,
  title = {A Comparison of Vector Symbolic Architectures},
  author = {Schlegel, Kenny and Neubert, Peer and Protzel, Peter},
  year = {2021},
  month = dec,
  journal = {Artificial Intelligence Review},
  issn = {0269-2821, 1573-7462},
  doi = {10.1007/s10462-021-10110-3},
  abstract = {Vector Symbolic Architectures combine a high-dimensional vector space with a set of carefully designed operators in order to perform symbolic computations with large numerical vectors. Major goals are the exploitation of their representational power and ability to deal with fuzziness and ambiguity. Over the past years, several VSA implementations have been proposed. The available implementations differ in the underlying vector space and the particular implementations of the VSA operators. This paper provides an overview of eleven available VSA implementations and discusses their commonalities and differences in the underlying vector space and operators. We create a taxonomy of available binding operations and show an important ramification for non self-inverse binding operations using an example from analogical reasoning. A~main contribution is the experimental comparison of the available implementations in order to evaluate (1) the capacity of bundles, (2) the approximation quality of non-exact unbinding operations, (3) the influence of combining binding and bundling operations on the query answering performance, and (4) the performance on two example applications: visual place- and language-recognition. We expect this comparison and systematization to be relevant for development of VSAs, and to support the selection of an appropriate VSA for a particular task. The implementations are available.},
  langid = {english},
  file = {/home/ross/Zotero/storage/IH8AQKFM/Schlegel et al. - 2021 - A comparison of vector symbolic architectures.pdf}
}

